Ancestor Embedder: None
Descendant Embedder: None
Likelihood model: neural_hmm/base_hmm_load_all
Normalizing losses by: desc_len

2: model init

3: main training loop
New best test loss at epoch 0: 24.694913864135742

4: post-training actions
Regular stopping after 0 full epochs:

Epoch with lowest average test loss ("best epoch"): 0

RE-EVALUATING ALL DATA WITH BEST PARAMS:

SCORING ALL TRAIN SEQS

SCORING ALL TEST SEQS

TRAIN SET:
==========
final_ave_loss: 98.77965545654297
final_ave_loss_seqlen_normed: 24.694913864135742
final_perplexity: 53071921152.0
final_ece: 53071921152.0

TEST SET:
==========
final_ave_loss: 98.77965545654297
final_ave_loss_seqlen_normed: 24.694913864135742
final_perplexity: 53071921152.0
final_ece: 53071921152.0
