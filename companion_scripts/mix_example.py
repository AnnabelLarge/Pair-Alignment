"""
example of an arbitrarily-nested mixture model, generated by 
  chatGPT; this would take some work to move to jax and make
  jit-compatible

maybe use this as a template for nested TKF92? Unclear if I need that 
  kind of flexibility though...
"""


import numpy as np

class MixtureModel:
    def __init__(self, components, weights):
        """
        Recursive mixture model.

        Parameters
        ----------
        components : list
            Each element can be either:
              - a distribution object with a `.log_prob(x)` method
              - another MixtureModel
        weights : np.ndarray
            Array of mixture weights (not necessarily normalized).
        """
        self.components = components
        self.weights = np.array(weights, dtype=float)
        self.weights /= self.weights.sum()  # normalize

    def log_prob(self, x):
        """
        Compute log probability for data point x.
        """
        log_probs = []
        for comp in self.components:
            # recursively evaluate log_prob
            lp = comp.log_prob(x) if isinstance(comp, MixtureModel) else comp.log_prob(x)
            log_probs.append(lp)

        # mixture log-sum-exp
        log_probs = np.array(log_probs)
        return np.log(np.sum(self.weights * np.exp(log_probs)))
    

# Example: simple normal distribution wrapper
class NormalDist:
    def __init__(self, mu, sigma):
        self.mu = mu
        self.sigma = sigma

    def log_prob(self, x):
        return -0.5 * np.log(2*np.pi*self.sigma**2) - 0.5 * ((x - self.mu)/self.sigma)**2


# Example usage:
leaf1 = NormalDist(0, 1)
leaf2 = NormalDist(5, 1)

# First-level mixture
mix1 = MixtureModel([leaf1, leaf2], [0.3, 0.7])

# Nested mixture: another mixture that combines mix1 with a new normal
leaf3 = NormalDist(10, 2)
nested_mix = MixtureModel([mix1, leaf3], [0.5, 0.5])

print("log p(x=1) under nested mix:", nested_mix.log_prob(1))
